% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/lgb_fit.R
\name{lgb_fit}
\alias{lgb_fit}
\title{Wrapper for lgb.train.}
\usage{
lgb_fit(data = data, target = target, split = split,
  preproc_fun = preproc_fun, params = params, args = args,
  metrics = metrics, return_val_preds = FALSE,
  return_model_obj = FALSE, ...)
}
\arguments{
\item{data}{data.table with all input data.}

\item{target}{Target variable name (character).}

\item{split}{Indicator variable with 1 corresponds to observations in validation dataset.}

\item{preproc_fun}{Preprocessing function which takes data.table \code{data}+\code{split}
as input and returns processed data.table with same \code{target} and \code{split} columns.}

\item{params}{1-row data.table with all hyperparameters.}

\item{args}{List with parameters unchangeable during tuning.}

\item{metrics}{Vector of metric functions names.}

\item{return_val_preds}{If \code{TRUE}, predictions for validation data 
will be returned.}

\item{return_model_obj}{If \code{TRUE}, model object will be returned.}

\item{...}{Other parameters for \code{kgb.train()}.}

\item{train_on_all_data}{If \code{TRUE}, model will be fitted on all data
(without train/validation split) and model object will be returned.}
}
\value{
data.table with optimal number of iterations (implies that we use early stopping)
and all metrics calculated for validation part of the data. It also contains 
predictions for validation data if \code{return_val_preds = TRUE}.
}
\description{
Fit and evaluate lightgbm model with data.table as input data.
Model are trained (including all preprocessing steps) on train part and
evaluated on validation part according to \code{split} indicator variable.
}
\details{

}
\examples{
# Input data
dt <- as.data.table(mtcars)
# data.table with resamples
splits <- resampleR::cv_base(dt, "hp")
# data.table with all hyperparameters
    lgb_grid <- CJ(
    learning_rate = 0.03, 
    metric = "rmse",
    num_leaves = 30,
    verbose = 1,
    subsample = 0.9,
    colsample_bytree = 0.8,
    random_state = 42,
    max_depth = c(3, 5, 7),
    lambda_l2 = 0.02,
    lambda_l1 = 0.004,
    bagging_fraction = 0.8,
    feature_fraction = 0.7,
    min_child_samples = 3,
    verbose = -1
)
# Non-tunable parameters for lightgbm
lgb_args <- list(
    nrounds = 1000,
    obj = "regression",
    early_stopping_rounds = 10,
    verbose = -1
)
# Dumb preprocessing function
# Real function will contain imputation, feature engineering etc.
# with all statistics computed on train folds and applied to validation fold
preproc_fun_example <- function(data) return(data[])
lgb_fit(data = dt,
        target = "hp",
        split = splits[, split_1],
        preproc_fun = preproc_fun_example,
        params = lgb_grid[1, ],
        args = lgb_args,
        metrics = c("rmse", "mae"),
        return_val_preds = TRUE)

}
