% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/xgb_fit.R
\name{xgb_fit}
\alias{xgb_fit}
\title{Wrapper for xgb.train.}
\usage{
xgb_fit(data = data, target = target, split = split,
  preproc_fun = preproc_fun, params = params, args = args,
  metrics = metrics, return_val_preds = FALSE,
  return_model_obj = FALSE, train_on_all_data = FALSE, ...)
}
\arguments{
\item{data}{data.table with all input data.}

\item{target}{Target variable name (character).}

\item{split}{Indicator variable with 1 corresponds to observations in validation dataset.}

\item{preproc_fun}{Preprocessing function which takes data.table \code{data}+\code{split}
as input and returns processed data.table with same \code{target} and \code{split} columns.}

\item{params}{1-row data.table with tunable hyperparameters.}

\item{args}{List with parameters unchangeable during tuning.}

\item{metrics}{Vector of metric functions names.}

\item{return_val_preds}{If \code{TRUE}, predictions for validation data 
will be returned.}

\item{return_model_obj}{If \code{TRUE}, model object will be returned.}

\item{train_on_all_data}{If \code{TRUE}, model will be fitted on all data
(without train/validation split) and model object will be returned.}

\item{...}{Other parameters for \code{xgb.train()}.}
}
\value{
data.table with optimal number of iterations (implies that we use early stopping)
and all metrics calculated for validation part of the data. It also contains 
predictions for validation data if \code{return_val_preds = TRUE}.
}
\description{
Fit and evaluate xgboost model with data.table as input data.
Model are trained (including all preprocessing steps) on train part and
evaluated on validation part according to \code{split} indicator variable.
}
\details{

}
\examples{
# Input data
dt <- as.data.table(mtcars)
# data.table with resamples
splits <- resampleR::cv_base(dt, "hp")
# data.table with tunable model hyperparameters
xgb_grid <- CJ(
    max_depth = c(6, 8),
    eta = 0.025,
    colsample_bytree = 0.9,
    subsample = 0.8,
    gamma = 0,
    min_child_weight = c(3, 5),
    alpha = 0,
    lambda = 1
)
# Non-tunable parameters for xgboost
xgb_args <- list(
    nrounds = 500,
    early_stopping_rounds = 10,
    booster = "gbtree",
    eval_metric = "rmse",
    objective = "reg:linear",
    verbose = 0
)
# Dumb preprocessing function
# Real function will contain imputation, feature engineering etc.
# with all statistics computed on train folds and applied to validation fold
preproc_fun_example <- function(data) return(data[])
xgb_fit(data = dt,
        target = "hp",
        split = splits[, split_1],
        preproc_fun = preproc_fun_example,
        params = xgb_grid[1, ],
        args = xgb_args,
        metrics = c("rmse", "mae"))

}
